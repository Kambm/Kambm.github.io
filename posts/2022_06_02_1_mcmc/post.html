<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset='UTF-8'/>
	<title>Inverting Options Data for Market Expectations</title>
	<link rel="stylesheet" href="https://kambm.github.io/styles/base.css">	
	<link href="https://fonts.googleapis.com/css?family=Bellefair|Cormorant+Infant|&display=swap" rel="stylesheet"> 
	<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/
	css2?family=Merriweather+Sans:wght@600&display=swap" rel="stylesheet">
	<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body>
 	<div class="site-header">
	<h3 style="margin:0px">
		<a class="name-title" href="https://kambm.github.io/index.html">Mason Kamb</a>	</h3>
	<nav class="site-menu">
		<h5 class="thingy">
			
			<a href="https://kambm.github.io/index.html" class="navbar-item">About</a>
			<a href="https://kambm.github.io/research.html" class="navbar-item">Research</a>
			<a href="https://kambm.github.io/Kambm.github.io/vitae.html" class="navbar-item">Vitae</a>
			<a href="https://kambm.github.io/blog.html" class="navbar-item">Blog</a>
					</h5>
	</nav>
</div>
 	<div class="main-text">

 		<h1 class="blog-title">
	 		Convergence properties of some chains in a quadratic basin 
	 	</h1>

<h2 id="introduction">Introduction</h2>
<p>Consider the following stochastic ODE: <span
class="math display">\[\begin{aligned}
    d\textbf{x}_t &amp;= -\textbf{A}\textbf{x}_t\,dt +
\sqrt{2\beta^{-1}} d\textbf{W}_t\end{aligned}\]</span> The Fokker-Planck
equation this process induces is <span
class="math display">\[\begin{aligned}
    \partial_t \rho &amp;= \nabla\cdot(\textbf{A}\textbf{x} \rho) +
\beta^{-1} \nabla^2 \rho = \text{Tr}[\textbf{A}] \rho +
\textbf{A}\textbf{x} \cdot \nabla \rho + \beta^{-1} \nabla^2
\rho\end{aligned}\]</span> The invariant distribution associated with
this equation is <span class="math display">\[\begin{aligned}
    \tilde{\rho} &amp;= \exp(-\beta \textbf{x}^\dagger \textbf{C}^{-1}
\textbf{x})\end{aligned}\]</span> where <span
class="math inline">\(\textbf{C}\)</span> solves the Lyapunov equation
<span class="math inline">\(\textbf{A}\textbf{C} +
\textbf{C}\textbf{A}^\dagger = 2\textbf{I}\)</span>. We would like to
solve the full Fokker-Planck equation for this system. To do this we
will first quotient out the invariant distribution and write the
equation for <span class="math inline">\(h =
\frac{\rho}{\tilde{\rho}}\)</span>: <span
class="math display">\[\begin{aligned}
\label{eq:heq}
    \partial_t h &amp;=
h(\text{Tr}[\textbf{C}^{-1}]-\text{Tr}[\textbf{A}] - \beta
\textbf{x}^\dagger \textbf{A}^\dagger \textbf{C}^{-1}\textbf{x} + \beta
\textbf{x}^\dagger \textbf{C}^{-2} \textbf{x}) + \nabla h \cdot
[-2\textbf{C}^{-1}\textbf{x} + \textbf{A}\textbf{x}] +
\beta^{-1}\nabla^{2}h\end{aligned}\]</span> When <span
class="math inline">\(\textbf{A} = \textbf{A}^\dagger\)</span> and thus
<span class="math inline">\(-\textbf{A}\textbf{x}\)</span> is the
negative gradient of a potential, <span
class="math inline">\(\textbf{C}^{-1} = \textbf{A}\)</span> and this
equation reduces to <span class="math display">\[\begin{aligned}
\label{eq:hsimple}
    \partial_t h &amp;= -\nabla h \cdot \textbf{A}\textbf{x} +
\beta^{-1}\nabla^2 h\end{aligned}\]</span> This is a linear equation,
and in order to solve it we will try to obtain the eigenvalues and
eigenfunctions. This is very easy to do in this case as these are all
polynomial functions. The ground state <span class="math inline">\(h(x)
= c\)</span> is an eigenfunction associated with eigenvalue zero. For
eigenfunctions of the next order polynomial order, <span
class="math inline">\(\textbf{w}^\dagger \textbf{x}\)</span>, the
equation is <span class="math display">\[\begin{aligned}
    \lambda \textbf{w}^\dagger \textbf{x} &amp;= - \textbf{w}^\dagger
\textbf{A}\textbf{x}\end{aligned}\]</span> which entails that <span
class="math inline">\(\textbf{w}\)</span> are the eigenvectors of <span
class="math inline">\(\textbf{A}\)</span> and <span
class="math inline">\(\lambda\)</span> the negatives of the associated
eigenvalues of <span class="math inline">\(\textbf{A}\)</span>. These
eigenvalues are the largest nonzero eigenvalues of our system and thus
dictate the rate of asymptotic convergence to the stationary
distribution.</p>
<p>Suppose we now perturb our dynamical matrix <span
class="math inline">\(\textbf{A}&#39; = \textbf{A} - \textbf{T}\)</span>
with a perturbation <span class="math inline">\(\textbf{T}\)</span> that
leaves <span class="math inline">\(\textbf{C}\)</span> invariant. This
gives the following condition on <span
class="math inline">\(\textbf{T}\)</span>: <span
class="math display">\[\begin{aligned}
    2\textbf{I} &amp;= (\textbf{A} + \textbf{T})\textbf{A}^{-1} +
\textbf{A}^{-1} (\textbf{A} + \textbf{T}^\dagger) \\
    \implies 0 &amp;= \textbf{T}\textbf{A}^{-1} + \textbf{A}^{-1}
\textbf{T}^\dagger\end{aligned}\]</span> Since <span
class="math inline">\(\textbf{T}^\dagger \textbf{A} =
(\textbf{A}\textbf{T})^\dagger\)</span>, we see that <span
class="math inline">\(\textbf{T}^\dagger \textbf{A}\)</span> is
antisymmetric. One consequence of this is that the flow induced by <span
class="math inline">\(\textbf{T}\)</span> at a point <span
class="math inline">\(\textbf{x}\)</span>, given by <span
class="math inline">\(\textbf{T}\textbf{x}\)</span>, is orthogonal to
the gradient <span class="math inline">\(\textbf{A}\textbf{x}\)</span>
at that point. We will look at which linear transformations satisfy this
condition later; for now, we substitute <span
class="math inline">\(\textbf{T} - \textbf{A}\)</span> into our
equation, which gives <span class="math display">\[\begin{aligned}
    \partial_t h &amp;= -h(\textbf{x}^\dagger \textbf{T}^\dagger
\textbf{A} \textbf{x}) + \nabla h
\cdot[(\textbf{T}-\textbf{A})\textbf{x}] + \beta^{-1} \nabla^2
h\end{aligned}\]</span> Since <span
class="math inline">\(\textbf{T}^\dagger \textbf{A}\)</span> is
antisymmetric, the first term vanishes and we are left with <span
class="math display">\[\begin{aligned}
    \partial_t h &amp;= \nabla h \cdot [(\textbf{T}-
\textbf{A})\textbf{x}] +\beta^{-1}\nabla^2 h\end{aligned}\]</span> This
is in essentially the same form as before. In this case the eigenvalues
associated with the lowest-order eigenfunctions of are the eigenvalues
of <span class="math inline">\(\textbf{T} - \textbf{A}\)</span>. The
rate of convergence is dictated by the real part of these eigenvalues.
We would like to know if the perturbation <span
class="math inline">\(\textbf{T}\)</span> increases this rate, i.e.
makes the largest nonzero real part of the eigenvalues smaller.</p>
<p>What linear transformations satisfy our antisymmetry condition? In
two dimensions it is very clear: <span class="math inline">\(\textbf{T}
= \epsilon\textbf{R} \textbf{A}\)</span>, where <span
class="math inline">\(\textbf{R}\)</span> is a <span
class="math inline">\(\pi/2\)</span> rotation matrix of positive or
negative sign. In higher dimensions this is not as convenient: there is
always an invariant <span class="math inline">\(d-2\)</span> dimensional
subspace of a rotation matrix, and any <span
class="math inline">\(\textbf{x}\)</span> such that <span
class="math inline">\(\textbf{A}\textbf{x}\)</span> has a nonzero
projection onto that subspace will not have <span
class="math inline">\(\textbf{T}\textbf{x}\)</span> and <span
class="math inline">\(\textbf{A}\textbf{x}\)</span> orthogonal without
further correction to <span
class="math inline">\(\textbf{T}\textbf{x}\)</span>. The correction
here, however, is quite obvious: we simply project <span
class="math inline">\(\textbf{A}\textbf{x}\)</span> onto the subspace
orthogonal to the invariant subspace, and then rotate. This gives us
<span class="math inline">\(\textbf{T} = \epsilon \textbf{R}_{ S}
\textbf{P}_{\perp S} \textbf{A}\)</span>, where <span
class="math inline">\(\textbf{R}_{S}\)</span> is a <span
class="math inline">\(\pi/2\)</span> unitary transformation that leaves
the subspace <span class="math inline">\(S\)</span> invariant, and <span
class="math inline">\(\textbf{P}_{\perp S} = \textbf{I} -
\textbf{P}_{\textbf{S}}\)</span> is the antiprojection operator for this
subspace.</p>
<p>Are these the only matrices that satisfy this condition? The answer
is yes, which can be seen from a dimension counting argument. The
antisymmetry condition <span class="math inline">\(\textbf{T}^\dagger
\textbf{A} = -\textbf{A}\textbf{T}\)</span> reads <span
class="math inline">\(\textbf{t}_i^\dagger \textbf{a}_j =
-\textbf{a}_i^\dagger \textbf{t}_j\)</span> for each <span
class="math inline">\(i,j\)</span> pair. There are <span
class="math inline">\(d(d-1)/2\)</span> independent conditions here due
to the symmetry of <span class="math inline">\(\textbf{A}\)</span>, so
we expect the solution manifold to be <span class="math inline">\((d^2 -
d(d-1)/2)\)</span>-dimensional. And indeed, picking a single <span
class="math inline">\((d-2)\)</span> dimensional invariant subspace
<span class="math inline">\(S\)</span> is equivalent to picking 2
orthogonal <span class="math inline">\(d\)</span>-dimensional vectors,
which has the appropriate dimensionality for the problem.</p>
<h2 id="perturbation-theory">Perturbation theory</h2>
<p>We will first approach this question perturbatively, in order to
answer the basic question: can adding a small velocity orthogonal to the
gradient <em>ever</em> add a speed up? The first-order perturbation
equation is <span class="math display">\[\begin{aligned}
    (\epsilon \textbf{R}_{S} \textbf{P}_{\perp S} \textbf{A} -
\textbf{A}) (\textbf{v}_{\lambda}^0 + \epsilon \textbf{v}_{\lambda}^1)
&amp;= (\lambda_0 + \epsilon \lambda_1) (\textbf{v}_{\lambda}^0 +
\epsilon \textbf{v}_{\lambda}^1)\end{aligned}\]</span></p>
<p>The first-order in <span class="math inline">\(\epsilon\)</span>
corrections to the eigenvalues <span
class="math inline">\(\lambda\)</span> of <span
class="math inline">\(-\textbf{A}\)</span> are <span
class="math display">\[\begin{aligned}
    \lambda_1 =\langle \textbf{v}_{\lambda}^0, \textbf{R}_{S}
\textbf{P}_{\perp S} \textbf{A} \textbf{v}_{\lambda}^0 \rangle =
-\lambda_0 \langle \textbf{v}_{\lambda}^0, \textbf{R}_{S}
\textbf{P}_{\perp S}  \textbf{v}_{\lambda}^0 \rangle =
0\end{aligned}\]</span> The equation for <span
class="math inline">\(\textbf{v}_{\lambda}^1\)</span> is then given by
<span class="math display">\[\begin{aligned}
    -\lambda_0 \textbf{R}_{S} \textbf{P}_{\perp
S}  \textbf{v}_{\lambda}^0 = (\textbf{A} + \lambda_0)
\textbf{v}_{\lambda}^1\end{aligned}\]</span> While <span
class="math inline">\(\textbf{A} + \lambda_0\)</span> is singular, <span
class="math inline">\(\textbf{v}^1_\lambda\)</span> is orthogonal to its
nullspace (i.e. the span of <span
class="math inline">\(\textbf{v}^0_{\lambda}\)</span>). We can thus
invert for <span class="math inline">\(\textbf{v}_{\lambda}^1\)</span>:
<span class="math display">\[\begin{aligned}
    \textbf{v}_{\lambda}^1 &amp;= -\lambda_0 \sum_{\gamma \neq \lambda}
\frac{\langle\textbf{v}_{\gamma}^0, \textbf{R}_{S} \textbf{P}_{\perp
S}  \textbf{v}_{\lambda}^0\rangle}{\lambda_0 - \gamma_0}
\textbf{v}_{\gamma}^0\end{aligned}\]</span> To see the effects of <span
class="math inline">\(\textbf{T}\)</span> on the eigenvalues, we need to
pass to at least second order. The second-order perturbation equation is
<span class="math display">\[\begin{aligned}
    \textbf{R}_{S}\textbf{P}_{\perp S}\textbf{A} \textbf{v}_{\lambda}^1
- \textbf{A} \textbf{v}_{\lambda}^2 &amp;= \lambda_2
\textbf{v}_\lambda^0 + \lambda_0
\textbf{v}_{\lambda}^2\end{aligned}\]</span> Taking the inner product
with <span class="math inline">\(\textbf{v}_\lambda^0\)</span> on both
sides and then subtracting like terms yields <span
class="math display">\[\begin{aligned}
    \lambda_2 &amp;= \langle
\textbf{v}_\lambda^0,   \textbf{R}_{S}\textbf{P}_{\perp S} \textbf{A}
\textbf{v}_{\lambda}^1 \rangle\\
    &amp;= -\lambda_0 \sum_{\gamma \neq \lambda} \frac{\langle
\textbf{v}_{\gamma}^0, \textbf{R}_{S}\textbf{P}_{\perp S}
\textbf{v}_{\lambda}^0 \rangle \langle \textbf{v}_\lambda^0,
\textbf{R}_{S}\textbf{P}_{\perp S} \textbf{A}
\textbf{v}_{\gamma}^0\rangle }{\lambda_0 - \gamma_0}\\
    &amp;= \lambda_0 \sum_{\gamma \neq \lambda} \gamma_0 \frac{\langle
\textbf{v}_{\gamma}^0, \textbf{R}_{S}\textbf{P}_{\perp S}
\textbf{v}_{\lambda}^0 \rangle \langle \textbf{v}_\lambda^0,
\textbf{R}_{S}\textbf{P}_{\perp S} \textbf{v}_{\gamma}^0\rangle
}{\lambda_0 - \gamma_0}\end{aligned}\]</span> <span
class="math inline">\(\textbf{R}_S \textbf{P}_{\dagger S}\)</span> is
anti-Hermitian, and thus the term <span class="math inline">\(\langle
\textbf{v}_{\gamma}^0, \textbf{R}_{\pi/2} \textbf{v}_{\lambda}^0 \rangle
\langle \textbf{v}_\lambda^0, \textbf{R}_{\pi/2}
\textbf{v}_{\gamma}^0\rangle\)</span> is negative or zero. For the
largest eigenvalue <span class="math inline">\(\lambda_0\)</span>, <span
class="math inline">\(\lambda_0\)</span> is negative, <span
class="math inline">\(\gamma_0\)</span> is negative, <span
class="math inline">\(\lambda_0 - \gamma_0\)</span> is positive, and the
numerator is negative, so the overall sign of the perturbation term is
negative. Thus there is a regime wherein any sufficiently small
perturbation of the appropriate form will, at least, not decrease the
rate of convergence.</p>
<h2 id="global-analysis-n-d-and-2d">Global analysis: <span
class="math inline">\(n\)</span>-d and 2d</h2>
<p>Consider the eigenvalue equation for the perturbed system: <span
class="math display">\[\begin{aligned}
    (\epsilon \textbf{T} - \textbf{A}) \textbf{v}_{\epsilon} &amp;=
\lambda_\epsilon \textbf{v}_\epsilon\end{aligned}\]</span> We multiply
both sides on the right by <span
class="math inline">\(\textbf{A}\)</span>, then apply <span
class="math inline">\(\textbf{v}_{\epsilon}^\dagger\)</span>. Since
<span class="math inline">\(\textbf{A}\textbf{T}\)</span> is
antisymmetric, the term involving <span
class="math inline">\(\textbf{v}^\dagger_\epsilon \textbf{A}\textbf{T}
\textbf{v}_\epsilon\)</span> must be zero or purely imaginary, and we
are left with <span class="math display">\[\begin{aligned}
    \mathbb{Re}[\lambda_{\epsilon}] &amp;=
-\frac{\textbf{v}_{\epsilon}^\dagger \textbf{A}^2
\textbf{v}_{\epsilon}}{\textbf{v}_{\epsilon}^\dagger \textbf{A}
\textbf{v}_{\epsilon}}\end{aligned}\]</span> From this we get a global
bound on the eigenvalues we can obtain via an orthogonal perturbation:
<span class="math display">\[\begin{aligned}
    \mathbb{Re}[\lambda_{\epsilon}] \leq
-\gamma_{\text{max}}\end{aligned}\]</span> where <span
class="math inline">\(\gamma_{\text{max}}\)</span> is the largest
eigenvalue of <span class="math inline">\(\textbf{A}\)</span>.</p>
<p>We can get a more detailed picture in two dimensions. Let us work in
the basis of the eigenvectors of <span
class="math inline">\(\textbf{A}\)</span>, with <span
class="math inline">\(\lambda^0\)</span> being the larger of the two. We
then have <span class="math display">\[\begin{aligned}
    (\epsilon \textbf{T} - \textbf{A}) &amp;= \begin{pmatrix}
    -\lambda^0 &amp; -\epsilon \lambda^1\\
    \epsilon \lambda^0 &amp; -\lambda^1
    \end{pmatrix}\end{aligned}\]</span> The characteristic polynomial is
then <span class="math display">\[\begin{aligned}
    0 &amp;= (\lambda_\epsilon + \lambda^0)(\lambda_\epsilon +
\lambda^1) + \epsilon^2 \lambda^1 \lambda^0\\
    &amp;= \lambda_\epsilon^2 + (\lambda^0 + \lambda^1)\lambda_\epsilon
+ (1 + \epsilon^2) \lambda^1 \lambda^0\end{aligned}\]</span> which in
turn implies <span class="math display">\[\begin{aligned}
    \lambda_\epsilon &amp;= -\frac{\lambda^0 + \lambda^1}{2} \pm
\frac{\sqrt{(\lambda^0 + \lambda^1)^2 - 4(1+
\epsilon^2)\lambda^1\lambda^0}}{2}\end{aligned}\]</span> The eigenvalue
with the maximal real part is given by the <span
class="math inline">\(+\)</span> option. This value is strictly
decreasing with <span class="math inline">\(\epsilon\)</span>, until the
discriminant is negative, at which point the largest real part is given
by <span class="math inline">\(-\frac{\lambda_0 +
\lambda_1}{2}\)</span>. This implies that the maximal speedup is only
<span class="math inline">\(\frac{1}{2}(\lambda_0 - \lambda_1)\)</span>,
and not <span class="math inline">\(\lambda_0 - \lambda_1\)</span> as
the bound would imply.</p>
<p>In higher dimensions things are not quite as convenient to analyze,
but we give a simple heuristic argument for how the eigenvalues should
scale. Instead of working in the eigenbasis of <span
class="math inline">\(\textbf{A}\)</span>, we will consider a basis such
that the invariant subspace <span class="math inline">\(S\)</span> and
its orthogonal complement <span class="math inline">\(\perp S\)</span>
are spanned by independent basis vectors. In such a representation, we
can write <span class="math display">\[\begin{aligned}
    \epsilon \textbf{T} - \textbf{A} &amp;= \begin{pmatrix}
     (\epsilon \textbf{r} - \textbf{I})\textbf{A}_{\perp S, \perp S}
&amp; -\textbf{A}_{\perp S , S}\\
     -\textbf{A}_{S, \perp S} &amp; -\textbf{A}_{S, S}
    \end{pmatrix}\end{aligned}\]</span> where <span
class="math display">\[\begin{aligned}
    \textbf{r} = \begin{pmatrix}
    0 &amp; - 1\\
    1 &amp;0
    \end{pmatrix}\end{aligned}\]</span> Immediately we notice that the
upper block matrix has the same eigenvalues and eigenvectors as the 2d
problem we solved previously. Since this represents the component where
the horizontal flow is enhanced, we suggest (heuristically) that the
best maximal eigenvalue for <span class="math inline">\(\epsilon
\textbf{T} - \textbf{A}\)</span> is obtained by choosing <span
class="math inline">\(S\)</span> to be the span of the maximal and
minimal eigenvectors of <span
class="math inline">\(-\textbf{A}\)</span>, yielding the same averaging
behavior as before. Of course, the works only up to a point; in the
heuristic regime the best maximal eigenvalue is the maximum of this
value, an the second largest eigenvalue of <span
class="math inline">\(-\textbf{A}\)</span>.</p>
<h2 id="hamiltonian-monte-carlo">Hamiltonian Monte Carlo</h2>
<p>The above system is a toy model that helps us understand the benefits
of moving along the level sets of the invariant distribution. However,
this is not typically the algorithm used in practice to exploit the
speedups conferred by this phenomenon. A more commonly used algorithm is
Hamiltonian Monte Carlo (HMC). HMC adds auxiliary momenta <span
class="math inline">\(\textbf{p}_t\)</span> to the state variables <span
class="math inline">\(\textbf{x}_t\)</span>, and stochastically force
the momenta instead of the the state.</p>
<p>We start by defining a Hamiltonian for the momentum and position. We
will typically want it to be one where the momentum and the position are
uncoupled (although we will return to an example where this is not the
case later) so that the marginal distribution for <span
class="math inline">\(\textbf{x}\)</span> is easy to analyze, such as
the following: <span class="math display">\[\begin{aligned}
    H(\textbf{x},\textbf{p}) &amp;= U(\textbf{x}) + \frac{\lVert
\textbf{p} \rVert^2}{2m}\end{aligned}\]</span> We then define the chain
dynamics as follows: <span class="math display">\[\begin{aligned}
    d\textbf{x}_t &amp;= \nabla_\textbf{p} H\,dt\\
    d\textbf{p}_t &amp;= -\nabla_\textbf{x} H\,dt - \nabla
H_\textbf{p}\,dt + \sqrt{2\beta^{-1}}
d\textbf{W}_t\end{aligned}\]</span> We take as ansatz for the invariant
distribution <span class="math inline">\(\rho = \exp(-\beta H)\)</span>.
To verify this, we examine Fokker-Planck evolution of this distribution:
<span class="math display">\[\begin{aligned}
    \partial_t \rho &amp;= -\nabla_\textbf{x}\cdot[\nabla_\textbf{p} H
\rho] + \nabla_\textbf{p} \cdot[(\nabla_\textbf{x} H + \nabla_\textbf{p}
H)\rho] + \beta^{-1} \nabla_\textbf{p}^2 \rho\\
    &amp;= \rho [-\nabla_\textbf{x} \cdot \nabla_\textbf{p}H + \beta
\nabla_\textbf{p} H \cdot \nabla_\textbf{x} H] + \rho
[\nabla_\textbf{p}\cdot \nabla_\textbf{x} H - \beta \nabla_\textbf{p} H
\cdot \nabla_\textbf{x} H]\\
    &amp;+\rho[\nabla_\textbf{p}^2 H - \beta \nabla_\textbf{p} H \cdot
\nabla_\textbf{p} H] + \rho[\beta\nabla_\textbf{p} H\cdot
\nabla_\textbf{p} H-\nabla_\textbf{p}^2H ]\\
    &amp;= 0\end{aligned}\]</span> Thus our dynamics produce the correct
invariant distribution. Having verified this, we would now like to
analyze the convergence of HMC in a quadratic basin, as we did
previously for the case of linearly-perturbed gradient dynamics. As
before we pass to the equation for <span class="math inline">\(h =
\rho/\tilde{\rho}\)</span>: <span class="math display">\[\begin{aligned}
    \partial_t h &amp;= \gamma d h + \gamma \textbf{p} \cdot
\nabla_\textbf{p} h - \beta \gamma h \textbf{p} \cdot \nabla_\textbf{p}
H - \nabla_\textbf{p} H \cdot \nabla_\textbf{x} h + \nabla_\textbf{x}H
\cdot \nabla_\textbf{p} h \\
    &amp;- 2\nabla_\textbf{p} h \cdot \nabla_\textbf{p} H + h[\beta
\lVert \nabla_\textbf{p} H \rVert^2 - \nabla_\textbf{p}^2 H] +
\beta^{-1} \nabla_\textbf{p}^2 h\\
    &amp;= [\nabla U(\textbf{x}) -\frac{\textbf{p}}{m}] \cdot
\nabla_\textbf{p} h - \frac{\textbf{p}}{m} \cdot \nabla_\textbf{x} h +
\beta^{-1} \nabla_\textbf{p}^2 h\end{aligned}\]</span> For <span
class="math inline">\(U(\textbf{x}) = \frac{1}{2} \textbf{x}^\dagger
\textbf{A} \textbf{x}\)</span>, we can write this as <span
class="math display">\[\begin{aligned}
    \partial_t h(\textbf{s}) &amp;= \nabla_\textbf{s} h \cdot
\textbf{Q}\textbf{s} + \beta^{-1} \nabla_\textbf{p}^2
h\end{aligned}\]</span> where <span class="math inline">\(\textbf{s} =
(\textbf{x},\textbf{p})\)</span> and <span
class="math display">\[\begin{aligned}
    \textbf{Q} &amp;= \begin{bmatrix}
    \textbf{0} &amp; -m^{-1}\textbf{I} \\
    \textbf{A} &amp; -m^{-1}\textbf{I}
    \end{bmatrix}\end{aligned}\]</span> As before, we observe that the
convergence is likely dictated by the eigenfunctions of the form <span
class="math inline">\(h = \textbf{w}^\dagger \textbf{s}\)</span>,
leading to the eigenvalue equation <span
class="math display">\[\begin{aligned}
    \lambda\textbf{w}^\dagger \textbf{s} &amp;=
\textbf{w}^\dagger\textbf{Q} \textbf{s}\end{aligned}\]</span> Thus we
need to identify the eigenvalues and eigenvectors of <span
class="math inline">\(\textbf{Q}^\dagger\)</span>. The eigenvalue
equation for <span class="math inline">\(\textbf{Q}^\dagger\)</span> is
<span class="math display">\[\begin{aligned}
    0 &amp;= \begin{vmatrix}
    -\lambda \textbf{I} &amp; \textbf{A}\\
    -m^{-1}\textbf{I} &amp; -(m^{-1} + \lambda)\textbf{I}
    \end{vmatrix}\\
    &amp;= \det(\lambda(m^{-1} + \lambda) \textbf{I} + m^{-1}
\textbf{A})\end{aligned}\]</span> Thus we can obtain the eigenvalues of
<span class="math inline">\(\textbf{Q}\)</span> be solving the following
generalized eigenproblem: <span class="math display">\[\begin{aligned}
    \textbf{A}\textbf{v} &amp;= -m\lambda(m^{-1} + \lambda)
\textbf{v}\end{aligned}\]</span> For each eigenvalue <span
class="math inline">\(\kappa\)</span> of <span
class="math inline">\(\textbf{A}\)</span>, the associated eigenvalues of
<span class="math inline">\(\textbf{Q}\)</span> are <span
class="math display">\[\begin{aligned}
    \lambda &amp;= -\frac{1}{2m} \bigg(1 \pm \sqrt{1 -
4m\kappa}\bigg)\end{aligned}\]</span> Taking <span
class="math inline">\(\kappa_0\)</span> to be the minimal eigenvalue of
<span class="math inline">\(\textbf{A}\)</span>, we see that the maximal
eigenvalue of <span class="math inline">\(\textbf{Q}\)</span> is <span
class="math display">\[\begin{aligned}
    \lambda_{m} &amp;= \frac{1}{2m}\bigg(-1 +  \sqrt{1 -
4m\kappa_0}\bigg)\end{aligned}\]</span> When <span
class="math inline">\(m \to 0\)</span>, this approaches <span
class="math inline">\(\lambda_m = -\kappa_0\)</span>, recapitulating the
performance of naive stochastic gradient dynamics. However, as <span
class="math inline">\(m\)</span> increases, the performance of HMC gets
better. Optimal performance is obtained by taking <span
class="math inline">\(m = (4\kappa_0)^{-1}\)</span>. In this case, <span
class="math inline">\(\lambda_m = -2\kappa_0\)</span>. From this we find
that HMC can in principle double the rate of convergence in a quadratic
potential, compared to naive gradient dynamics.</p>
<p>We thus find that the relative performance of HMC vs. level
set-enhanced gradient dynamics depends on the condition number of <span
class="math inline">\(\textbf{A}\)</span>. When <span
class="math inline">\(\textbf{A}\)</span> is well-conditioned, the
level-set enhancement, which at best may average the rates of the
largest and smallest directions in the potential, does not provide much
speedup. However, when <span class="math inline">\(\textbf{A}\)</span>
is very ill-conditioned, the factor of 2 speedup provided by HMC may be
eclipsed by that provided by the level set-enhanced dynamics.</p>

 	</div>
 	<div class="my-footer">
	<hr>
	<p style="padding-bottom: 20px;">
	© Mason Kamb • 2022 • masonify at gmail dot com
	<p>
</div></body>

</html>
